\documentclass[a4paper,11pt]{article}

\usepackage{amsmath,amssymb,amsthm,amsopn}%,anysize}
\usepackage{/c/tduong/home/tex/harvard/harvard}
%\marginsize{3cm}{3cm}{2.5cm}{2.5cm}

\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\gvec}[1]{\boldsymbol{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\gmat}[1]{\boldsymbol{#1}}
\newcommand{\bmat}[4]{\begin{bmatrix} #1 & #2 \\ #3 & #4\end{bmatrix}}
\newcommand{\argmin}[1]{\underset{#1}{\mathrm{argmin}} \ }
\newcommand{\argmax}[1]{\underset{#1}{\mathrm{argmax}} \ }
\newcommand{\indi}[1]{\boldsymbol{1} \lbrace #1 \rbrace}

\def\jneqi{\substack{j=1 \\ j\neq i}}
\def\kneqj{\substack{k=1 \\ k\neq j}}
\def\HH{\mat{H}}
\def\G{\mat{G}}
\def\ks{\texttt{ks}}
\def\sm{\texttt{sm}}
\def\KernSmooth{\texttt{KernSmooth}}
\def\MSE{\mathrm{MSE}}
\def\MISE{\mathrm{MISE}}
\def\AMSE{\mathrm{AMSE}}
\def\AMISE{\mathrm{AMISE}}
\def\SAMSE{\mathrm{SAMSE}}
\def\SCV{\mathrm{SCV}}
\def\PI{\mathrm{PI}}
\def\NR{\mathrm{NR}}
\def\BCV{\mathrm{BCV}}
\def\LSCV{\mathrm{LSCV}}
\def\vecr{\vec{r}}
\def\vecx{\vec{x}}
\def\vecy{\vec{y}}
\def\vecX{\vec{X}}
\def\intr2{\int_{\boldsymbol{\mathbb{R}}^2}}

\DeclareMathOperator{\E}{\boldsymbol{\mathbb{E}}}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Prob}{\boldsymbol{\mathbb{P}}}
\DeclareMathOperator{\given}{\vert}
\DeclareMathOperator{\Bias}{Bias}
\DeclareMathOperator{\VEC}{vec}
\DeclareMathOperator{\VECH}{vech}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\dg}{dg}
\DeclareMathOperator{\diag}{diag}


%\VignetteIndexEntry{ks} 
\title{Using ks for bivariate kernel density estimation}
\author{Tarn Duong}

\begin{document}

\maketitle

\section{Introduction}
  
Kernel density estimation has become a popular tool for visualising 
the distribution of univariate data. 
See \cite{simonoff96}, for example, for an overview. 
However, the use of high-dimensional 
density estimation as a tool for exploratory data analysis 
appears relatively uncommon amongst practitioners. 
When multivariate kernel density estimation is considered it is usually
in the constrained context with diagonal bandwidth matrices, as 
implemented in \proglang{R} packages (e.g. \pkg{sm} and \pkg{KernSmooth}) for 
kernel smoothing.  
We introduce a new pakcage \pkg{ks} for kernel smoothing i 
$\ks$ is an \texttt{R} package for 
kernel smoothing for 1- to 6-dimensional data with unconstrained bandwidth
matrices. 
Currently kernel density estimation (KDE) and kernel discriminant
analysis (KDA) are available.  
This vignette focuses on KDE for the 2-dimensional case.



\section{Kernel density estimation}

Univariate kernel density estimation has received considerable 
attention in the literature, partly because of its practical utility, and 
partly because 
it provides a simple testing ground for learning about nonparametric smoothing.
Kernel density estimation for multivariate data has received significantly 
less attention. The lower level of interest in the multivariate context may be 
explained, to some extent, by the difficulties in viewing high dimensional density 
functions. For this reason, if no other,
the use of high-dimensional density estimation as a tool for exploratory 
data analysis  appears relatively uncommon amongst practitioners. 

Bivariate kernel density estimation sits at an important junction between 
the univariate and high-dimensional multivariate cases. From a 
practical standpoint, 
bivariate density  estimates have a utility and accessibility that 
is akin to that of their univariate cousins,
largely because they can be viewed using familiar perspective (`wire-frame') 
or contour plots. From a theoretical viewpoint, bivariate density estimation is an 
excellent setting for understanding aspects of multivariate 
kernel smoothing. There are 
important aspects of bivariate kernel estimation that have no 
univariate analogue (such as 
the orientation of the kernel functions) yet can be generalised 
to higher dimensional cases with relatively little effort.
 
For a bivariate sample $\vecX_1, \ldots, \vecX_n$ drawn from a density $f$, 
the kernel density estimate is defined by
$$ 
\hat{f} (\vecx; \HH) = n^{-1}\sum_{i=1}^n K_{\HH} ( \vecx - \vec{X}_i)
$$
where $\vecx = (x_1, x_2)^T$ and $\vec{X}_i = (X_{i1}, X_{i2})^T, i = 1, 2,  
\ldots, n$.  
Here 
$K(\vecx)$ is the bivariate kernel (which we assume to be a probability density 
function); $\HH$ 
is the bandwidth matrix which is symmetric and positive-definite; 
and $K_{\HH}(\vecx) = |\HH|^{-1/2} 
K( \HH^{-1/2} \vecx)$. The choice of $\HH$ is crucially important in determining 
the performance
of $\hat f$. Bivariate bandwidth selection is a difficult problem which may be 
simplified (at
the expense of flexibility) by imposing constraints on $\HH$. For example, 
$\HH$ may be restricted to the class of diagonal (positive-definite) matrices, or to 
the class of (positive) multiples
of the identity matrix. The merits of imposing restrictions on $\HH$ have 
been investigated by 
\citeasnoun{wand93}. These authors conclude that choosing a 
diagonal bandwidth matrix will sometimes 
be adequate, but that in other cases there is much to be gained by 
selecting a full (i.e.~unconstrained) 
bandwidth matrix. While the use of a full bandwidth matrix requires 
an additional smoothing parameter 
(in comparison to diagonal $\HH$), it permits arbitrary orientation of 
the kernel function. 


\section{Optimal bandwidth matrices}

In order to measure the performance of $\hat f$ we will (in common with the 
great majority of
researchers in this field) use the mean integrated squared error (MISE) criterion, 
$$
\MISE \, \hat{f}(\cdot; \HH)
= \E \int_{\mathbb{R}^2} [ \hat{f}(\vecx; \HH) - f(\vecx) ] ^2
\ d \vecx.
$$
Here $f$ denotes the target density, from which $\vec{X}_1, \ldots, \vec{X}_n$ are 
assumed to
be a random sample. Our aim in bandwidth selection is to estimate
$$\HH_\MISE = \underset{\HH}{\mathrm{argmin}} \
\MISE \,\hat{f}(\cdot; \HH),
$$
over the space of all symmetric, positive definite $2 \times 2$ 
matrices. It is
well known that the optimal bandwidth $\HH_{\MISE}$ does not have a closed form. 
In order to
make progress it is usual to employ an asymptotic analysis. It can be shown 
that (under conditions to be specified)
\begin{equation}
\MISE \, \hat{f}(\cdot; \HH) = \AMISE \, \hat{f}(\cdot; \HH) + o(n^{-1}
|\HH|^{-1/2} + \tr^2 \HH)
\label{eq:mise}
\end{equation}
where
\begin{equation}
\AMISE \,\hat{f}(\cdot; \HH) = n^{-1} |\HH| ^{-1/2}
R(K) + \tfrac{1}{4} \mu_2(K)^2 (\VECH^T \HH) \gmat{\Psi}_4(\VECH \, \HH)
\label{eq:amise}
\end{equation}
where $R(K) = \int_{\mathbb{R}^2} K(\vecx)^2 \ d \vecx$, $\mu_2(K) 
\mat{I} = \int_{\mathbb{R}^2} \vecx 
\vecx^T K(\vecx) \ d \vecx$ with $\mu_2(K) < \infty$ and $\mat{I}$ is the
$2 \times 2$ identity matrix; and vech is the vector half 
operator (see 
\citeasnoun[chapter 4]{wand95}). The $\gmat{\Psi}_4$ matrix is 
the $3 \times 3$ matrix 
given by
$$
\gmat{\Psi}_4 = \int_{\mathbb{R}^2} \VECH  (2D^2 f(\vecx) -
\mathrm{dg} \, D^2 f(\vecx)) \ \VECH^T (2D^2 f(\vecx) -
\mathrm{dg} \, D^2 f(\vecx)) \ d \vecx
$$ 
where $D^2 f(\vecx)$ is the Hessian matrix of $f$ with respect to $\vecx$ 
and $\mathrm{dg} \, \mat{A}$ is
 matrix
$\mat{A}$ with all of its non-diagonal elements set to zero. Sufficient conditions 
for 
the validity of the expansions defined by Equations (\ref{eq:mise}) and 
(\ref{eq:amise}) are
that all entries in $D^2 f(\vecx)$ are square integrable and all entries of 
$\HH \rightarrow 0$ 
and $n^{-1} |\HH|^{-1/2} \rightarrow 0,$ as $n \rightarrow \infty.$


If we introduce some more notation, we can explicitly state an expression for 
the matrix $\gmat{\Psi}_4$ in terms of its individual elements. Let $\vec{r} = 
(r_1, r_2)$ where the $r_1, r_2$ are non-negative integers. Let
$|\vec{r}| = r_1 + r_2,$ then the $\vecr$-th partial derivative of $f$ can 
be written as 
$$
f^{(\vec{r})} (\vecx) =
\frac{\partial^{|\vec{r}|}} {\partial^{r_1}_{x_1} \partial^{r_2}_{x_2}} f(\vecx)
$$
and the integrated density derivative functional is
$$
\psi_{\vec{r}} = \int_{\mathbb{R}^2} f^{(\vec{r})} (\vecx)
f(\vecx)\ d \vecx.
$$ 
Then we can show that 
\begin{equation}
\gmat{\Psi}_4 =
\begin{bmatrix}\psi_{40} & 2\psi_{31} & \psi_{22} \\
2\psi_{31} & 4\psi_{22} & 2\psi_{13} \\
\psi_{22} & 2\psi_{13} & \psi_{04}
\end{bmatrix}.
\label{Psi}
\end{equation}
(Note that the subscript 4 on $\gmat{\Psi}$ relates to the order of the 
derivatives involved.)

Equations (\ref{eq:amise}) and (\ref{Psi}) combine to give a 
tractable approximation,
AMISE, to the MISE.
We  make use of 
the tractability of AMISE by seeking
$$
\HH_\AMISE = \underset{\HH}{\mathrm{argmin}} \ \AMISE \,
\hat{f}(\cdot; \HH)
$$
rather than $\HH_\MISE$. Of course, the AMISE is a functional of the unknown
target density, through $\gmat{\Psi}_4$ and so remains unknown.



\section{Plug-in bandwidth selectors}
\label{sec:plugin}
Plug-in methods of selecting the bandwidth matrix are based on the AMISE. In
particular, we require pilot estimates of the
$\psi_{\vec{r}}$ functionals that can be `plugged-in' to provide an estimate
$\hat{\gmat{\Psi}}_4$. This in turn produces an estimate 
\begin{equation}
\PI (\HH) = n^{-1} |\HH| ^{-1/2}
R(K) + \tfrac{1}{4} \mu_2(K)^2 (\VECH^T \HH) \hat{\gmat{\Psi}} _4(\VECH \HH)
\label{eq:api}
\end{equation}
that can be numerically minimised to give the plug-in bandwidth matrix, 
$\hat{\HH}_\PI$.


In order to implement plug-in selection of $\HH$ we require pilot estimates of
the integrated density derivative functionals, $\psi_{\vec{r}}$. If we note that 
$\psi_{\vec{r}}= \E f^{(\vec{r})}(\vec{X})$ where $\vec{X}$ has density $f$,
then a natural estimator of $\psi_{\vec{r}}$ is
\begin{equation}
\hat{\psi}_{\vec{r}} (\G) = n^{-1}
\sum_{i=1}^n \hat{f}^{(\vec{r})} (\vec{X}_i; \G) =
n^{-2}\sum_{i=1}^{n}\sum_{j=1}^{n} K^{(\vec{r})}_{\G} (\vec{X}_i -
\vec{X}_j)
\label{psir} 
\end{equation}
where $\G$ is a pilot bandwidth matrix (usually different to $\HH$). 
This is known as the 
leave-in-diagonals estimator as it includes the non-stochastic $i = j$  terms. 
An important aspect of this pilot estimation is the choice of $\G$. The
finite sample properties of $\psi_{\vec{r}}$ as a function of $\G$ 
are intractable, but we can again make progress through asymptotic expansions.

\subsection{AMSE pilot bandwidth selectors}



We consider pilot bandwidth matrices of the form 
$\G = g^2 \mat{I}$. While this form of $\G$ may appear 
very restrictive
and, moreover, inappropriate for many data sets, the deficiencies of this 
approach are 
perhaps less extreme than might appear at first sight. In the first place, 
the data may be 
pre-transformed so as to improve the applicability of this form of  $\G$
(we return to this topic in Section \ref{sec:pre}). 
Secondly this choice of $\G$ does not affect 
convergence rates for 
the estimates $\hat{\psi}_{\vec{r}}$. 

Setting $\G = g^2 \mat{I}$, an asymptotic form for the mean square 
error (MSE) of $\hat{\psi}_{\vec{r}}$, from \citeasnoun{wand94}, is 
\begin{align*}
\AMSE \,\hat{\psi}_{\vec{r}}(g) &= 2n^{-2}
g^{-2|\vecr|-2} \psi_{\vec{0}}R(K^{(\vec{r})}) 
+ \bigg[ n^{-1} g^{-|\vecr|-2} K^{(\vec{r})}(\vec{0}) +
\frac{1}{2} g^2\mu_2(K) \sum_{i=1}^2 \psi_{\vec{r}+2\vec{e}_i}
\bigg]^2.
\label{psi-amse} 
\end{align*}
Thus we are seeking 
$$g_{\vecr, \AMSE} = \argmin{g>0} \AMSE \, \hat{\psi}_{\vec{r}}(g).$$

For most common kernels, including the normal kernel, if all the
elements of $\vecr$ are even then
$K^{(\vec{r})}(\vec{0})$ and $\psi_{\vec{r}+2\vec{e}_i}$ will be
of opposite sign, for $i = 1, 2$.  Then the bias terms
will cancel each other if $g$ is equal to
\begin{equation}
\label{eq:gamse_even} g_{\vec{r}, \AMSE} = \bigg[ \frac{-2
K^{(\vec{r})}(\vec{0})} { \mu_2(K) \big( \sum_{i=1}^2
\psi_{\vec{r}+2\vec{e}_i}\big) n} \bigg]^{1/(j+4)}.
\end{equation}
If at least one of the elements of $\vecr$ is odd then $K^{(\vec{r})}
(\vec{0}) = 0.$ In this case, we can find the minimum AMSE if $g$ is
equal to
\begin{equation}
\label{eq:gamse_odd} g_{\vec{r}, \AMSE} = \bigg[ \frac{2
\psi_{\vec{0}} (2 |\vec{r}| + 2) R(K^{(\vec{r})})} { \mu_2(K)^2
\big( \sum_{i=1}^2 \psi_{\vec{r}+2\vec{e}_i}\big)^2 n^2}
\bigg]^{1/(2j + 6)}.
\end{equation}


These expressions for the AMSE pilot bandwidths
$g_{\vecr, \AMSE}$ involve higher order $\psi_{\vecr}$
functionals.  This dependency continues for all $\vecr$ so we need
a way to resolve this problem. One convenient way is to use normal
reference approximations. This is just
\begin{equation}
\label{eq:psi-ns} \hat{\psi}_{\vec{r}}^{\NR} = (-1)^{|\vec{r}|}
\phi_{2\mat{S}}^{(\vec{r})} (\vec{0}).
\end{equation}
where $\mat{S}$ is the sample variance and $\phi_{\gmat{\Sigma}}(\cdot - \gvec{\mu})$
is the normal density with mean $\gvec{\mu}$ and variance $\gmat{\Sigma}$.   
So starting with
normal reference approximations of all $\psi_{\vecr}$ functionals for
a given order, we can proceed to find estimates of the lower
order $\psi_{\vecr}$ functionals.

This method of computing $\hat{\psi}_{\vecr}$ thus requires one pilot 
bandwidth for each functional.  This means that computing $\hat{\gmat{\Psi}}_4$
requires many separate 
pilot bandwidths.  The 
$\hat{\gmat{\Psi}}_4$ estimated in this element-wise 
way is not guaranteed to be positive
definite.   This estimator could be negative definite and
 would lead to no solution to the optimisation of the
$\PI(\HH)$ or it could be nearly singular and would lead
to numerical instabilities. Hence using appropriate estimators of each element
of a matrix will not necessarily lead to an appropriate estimator
of the matrix as a whole.
Positive-definiteness can be guaranteed by using a single, common pilot bandwidth
for all $\psi_{\vecr}$ functionals. 


\subsection{SAMSE pilot bandwidth selectors}
Modifying AMSE pilot selectors, we derive
a SAMSE (Sum of Asymptotic Mean Squared Error) pilot selector.
This type of selector has been specially devised to maintain the
positive definiteness of $\hat{\gmat{\Psi}}_4$ which is
crucial to the numerical minimisation of the plug-in criterion PI. This
method is also simpler and more parsimonious than AMSE
selectors.

Let  the $j$-th order SAMSE be 
\begin{align}
\SAMSE_j(g) 
= \sum_{\vecr: |\vecr| = j} \AMSE \,\hat{\psi}_{\vec{r}}(g) =  n^{-2}g^{-2j-4} A_1+
n^{-1}g^{-j} A_2 + \tfrac{1}{4} g^4 A_3
\end{align}
where $A_1$ and $A_3$ are positive constants and 
$A_2$ is a negative constant. 
This has a minimum at 
\begin{equation} 
\label{eq:gsamse} 
g_{j, \SAMSE} =
\bigg[ \frac{(4j + 8) A_2} {\big( -j A_2 
+ \sqrt{ j^2 A_2^2  + 
(8j + 16) A_1 A_3}\big)n}\bigg]^{1/(j+4)}.
\end{equation}
This is the $j$-th order SAMSE pilot bandwidth. The computation of
$A_1, A_2$ and $A_3$ can be found in \citeasnoun{duong03}. 


\subsection{R examples}
 
The \texttt{unicef} data set contains the number of deaths of children under 5
years of age per 1000 live births and the average life expectancy
(in years) at birth for 73 countries with GNI (Gross National
Income) less than 1000 US dollars per annum per capita. The scatter plot
is in Figure \ref{fig:unicef}. The aim of kernel density estimation
is to find a smooth estimate of the distribution of this data.

\setkeys{Gin}{width=0.4\textwidth}
\begin{figure}[!htp]
\begin{center}
<<fig=TRUE>>=
library(ks)
data(unicef)
plot(unicef)
@
\caption{Scatter plot for unicef data}
\label{fig:unicef}
\end{center}
\end{figure}
\setkeys{Gin}{width=0.7\textwidth}

\newpage
Use \texttt{Hpi} for 
full plug-in selectors and \texttt{Hpi.diag} for diagonal plug-in selectors.
There are two arguments which further specify the plug-in selector
used: \texttt{nstage} is the number of pilot estimation stages (1 or 2)
and \texttt{pilot} is the type of pilot estimation (``amse" or ``samse").
The other argument \texttt{pre} involves the pre-transformations outlined
in Section \ref{sec:pre}. 
<<>>=
(Hpi1 <- Hpi(x=unicef, nstage=1, pilot="amse", pre="scale"))
(Hpi2 <- Hpi(x=unicef, nstage=2, pilot="samse", pre="sphere"))
(Hpi3 <- Hpi.diag(x=unicef, nstage=2, pilot="amse", pre="scale")) 
@
To compute a kernel density estimate over grid for display, the 
command is \texttt{kde}.  
<<>>=
fhat1 <- kde(x=unicef, H=Hpi1)
fhat2 <- kde(x=unicef, H=Hpi2)
fhat3 <- kde(x=unicef, H=Hpi3)
@

\newpage
We can use the generic \texttt{plot} command to plot these
kernel density estimates. The plots are in Figure \ref{fig:pi}.
 
\begin{figure}[!htp]
\begin{center}
<<fig=TRUE>>=
layout(rbind(c(1,2), c(3,4)))
plot(fhat1, main="Plug-in: 1-stage, AMSE pilot")
plot(fhat2, main="Plug-in: 2-stage, SAMSE pilot")
plot(fhat3, main="Plug-in diag: 2-stage, AMSE pilot")
layout(1)
@
\caption{Kernel density estimates with plug-in selectors for unicef data}
\label{fig:pi}
\end{center}
\end{figure}

%\clearpage

\section{Cross validation bandwidth selectors}

Cross-validation selectors are the main alternative to plug-in selectors.
There are three main types of cross-validation selectors: 
least squares, biased and smoothed. 
To generalise least squares and biased cross-validation from one dimension
to multi-dimensions is fairly straightforward.  They are straightforward
primarily because they do not require pilot bandwidths. 
However smoothed cross-validation selectors \emph{do} require pilot bandwidths and 
to generalise the selection of these pilot bandwidths is not trivial.

\subsection{Least squares cross validation}
\label{sec:lscv}
The bivariate version of 
the least squares cross validation (LSCV) criterion of
\citeasnoun{rudemo82} and \citeasnoun{bowman84} is 
$$\LSCV (\HH) = \intr2 \hat{f}(\vec{x}; \HH)^2 \ d \vecx - 2n^{-1} \sum_{i=1}^n 
\hat{f}_{-i} (\vec{X}_i; \HH).$$
Here the leave-one-out estimator is 
$$\hat{f}_{-i} (\vec{x}; \HH) = (n-1)^{-1} 
\sum_{\jneqi}^n K_\HH (\vec{x} - \vec{X}_j) $$ 
and the LSCV selector $\hat{\HH}_\LSCV$ 
is the minimiser of $\LSCV(\HH)$.
This criterion attempts to estimate the MISE in a fairly directly manner since 
$\E \LSCV(\HH) =\MISE \,$ $\hat{f}(\cdot; \HH) - R(f).$   
Due to its unbiasedness, the LSCV selector is sometimes called
the unbiased cross validation (UCV) selector. 

The LSCV can be expanded to give:
$$
\LSCV(\HH) = n^{-1} R(K) |\HH|^{-1/2} + n^{-1} (n-1)^{-1} 
\sum_{i=1}^n \sum_{\jneqi}^n 
( K_\HH * K_\HH - 2K_\HH) (\vec{X}_i - \vec{X}_j) 
$$
where $*$ is the convolution operator.
For normal kernels, this expression simplifies further since 
$\phi_\HH * \phi_\HH = \phi_{2\HH}:$
\begin{equation}
\label{eq:lscv2}
\LSCV(\HH) = n^{-1} R(K) |\HH|^{-1/2} + n^{-1} (n-1)^{-1} 
\sum_{i=1}^n \sum_{\jneqi}^n 
(\phi_{2\HH} - 2\phi_\HH) (\vec{X}_i - \vec{X}_j). 
\end{equation}


\subsection{Biased cross validation}
The LSCV selector relies on estimating the MISE.  The approach taken by the
biased cross validation (BCV) selector relies on estimating the AMISE.
So likewise for the plug-in selectors in Section \ref{sec:plugin}, we need to
estimate $\gmat{\Psi}_4.$  Plug-in methods use a pilot bandwidth matrix/matrices 
that is/are independent of $\HH.$ 
For BCV, we set $\G = \HH$ and use slightly
different estimators.    
   
There are two versions of BCV, depending on the 
estimator of $\psi_{\vecr}, |\vecr| = 4,$  see \citeasnoun{sain94}. We can use
\begin{equation}
\label{eq:leave-out1}
\check{\psi}_{\vec{r}} (\HH) =
n^{-2} \sum_{i=1}^{n}\sum_{\jneqi}^{n} 
(K^{(\vec{r})}_\HH *  K_{\HH}) (\vec{X}_i - \vec{X}_j)
\end{equation}
or we could use 
\begin{equation}
\label{eq:leave-out2} \tilde{\psi}_{\vec{r}} (\HH) = n^{-1}
\sum_{i=1}^n \hat{f}_{-i}^{(\vec{r})} (\vec{X}_i; \HH) =
n^{-1} (n-1)^{-1} \sum_{i=1}^{n}\sum_{\jneqi}^{n} 
K^{(\vec{r})}_{\HH} (\vec{X}_i - \vec{X}_j).
\end{equation}
We use these, rather than the leave-in-diagonals estimator of Section
\ref{sec:plugin}, as we no longer seek to annihilate the contribution
from the non-stochastic terms with the leading term of the 
leave-out-diagonals double sum.  
The estimates $\check{\gmat{\Psi}}_4$ and
$\tilde{\gmat{\Psi}}_4$ are obtained from $\gmat{\Psi}_4$ by substituting
$\check{\psi}_{\vecr}$ and $\tilde{\psi}_{\vecr}$  for $\psi_{\vecr}.$

The BCV1 function is the version of BCV with $\check{\gmat{\Psi}}_4$ 
\begin{equation}
\label{eq:bcv1}
\BCV 1 (\HH) =  n^{-1} R(K)|\HH|^{-1/2} + \tfrac{1}{4} \mu_2(K)^2 (\VECH^T \HH)
\check{\gmat{\Psi}}_4  (\VECH \HH)
\end{equation}
and the BCV2 function is the version with $\tilde{\gmat{\Psi}}_4$ 
\begin{equation}
\label{eq:bcv2}
\BCV 2 (\HH) =  n^{-1} R(K)|\HH|^{-1/2} + \tfrac{1}{4} \mu_2(K)^2 (\VECH^T \HH)
\tilde{\gmat{\Psi}}_4  (\VECH \HH).
\end{equation}
The BCV selectors $\hat{\HH}_\BCV$ are the minimisers of the appropriate 
BCV function.

\subsection{Smoothed cross validation}
\label{sec:scv}
Smoothed cross validation (SCV), introduced by \citeasnoun{hall92}, 
can be thought of as a hybrid of LSCV and BCV.  The SCV 
criterion takes the asymptotic integrated variance from the BCV but attempts
to estimate the integrated squared bias exactly rather than using its asymptotic
form:
\begin{align*}
\SCV (\HH) &= n^{-1}R(K) |\HH|^{-1/2} + n^{-2} \sum_{i=1}^n
\sum_{j=1}^n (K_\HH * K_\HH * L_\G * L_\G \\
&\quad -  2K_\HH * L_\G * L_\G + L_\G * L_\G)  (\vec{X}_i - \vec{X}_j)
\end{align*}
where $L_\G(\cdot)$ is the pilot kernel with pilot bandwidth matrix $\G.$
The SCV selector $\hat{\HH}_\SCV$ is the minimiser of $ \SCV(\HH).$
If $K = L = \phi$ then the SCV has a simpler form:
\begin{equation}
\label{eq:scv_norm}
\SCV (\HH) = n^{-1}|\HH|^{-1/2} (4\pi)^{-d/2} + n^{-2} \sum_{i=1}^n
\sum_{j=1}^n (\phi_{2\HH + 2\G} - 2\phi_{\HH + 2\G} + \phi_{2\G})
(\vec{X}_i - \vec{X}_j). 
\end{equation}


Now we have a similar problem to plug-in selectors: how to select
the pilot bandwidth $\G$. \citeasnoun{sain94} set the pilot to be equal to the
final bandwidth $\HH$.  This circumvents the need to select a separate pilot
bandwidth but this is sub-optimal. 
We generalise the process of \citeasnoun{jones91b} to find an optimal 
pilot bandwidth selector.


The criterion that we use to measure the discrepancy between $\hat{\HH}_\SCV$
and $\HH_\AMISE$ is  
$$\tr \MSE(\VECH \hat{\HH}_\SCV; \G) = \sum_{i=1}^d \E ( \hat{h}^2_{\SCV, i}
- h^2_{\AMISE, i})^2 $$
where $\hat{h}^2_{\SCV, i}$ is the $i$-th diagonal element of $\hat{\HH}_\SCV$
and $h^2_{\AMISE, i}$ is the $i$-th diagonal element of $\HH_\AMISE$.
This is a multivariate extension of the univariate $\MSE (\hat{h}) = 
\E (\hat{h} - h_\AMISE)^2$. 

As in Section \ref{sec:plugin}, we will use the parameterisation 
$g^2 \mat{I}$ for the pilot bandwidth matrix $\G$ i.e.  we
wish to find $$ g_\AMSE = \argmin{g>0} \tr \AMSE(\VECH \hat{\HH}_\SCV; g).$$  
We can show, after considerable effort, that for $d > 1$, 
\begin{equation}
g_\AMSE = \left[ \frac{12 C_{\mu_2}}
 { (-4 C_{\mu_1} + C_{\mu_0}^{1/2}) n }\right] ^{1/6}
\label{eq:gamse}
\end{equation}
where $C_{\mu_0}, C_{\mu_1}, C_{\mu_2}$ are constants.  The details of the derivation
of this pilot bandwidth is available in \citeasnoun{duong05}.

\subsection{R examples}
We continue with the \texttt{unicef} data. 
\texttt{Hlscv} and \texttt{Hlscv.diag} are the full and diagonal
LSCV selectors. (The \texttt{Hstart} argument in the below code specifies the initial
value for the numerical minimisation - it can be specified
for any of the bandwidths in \ks). 
\texttt{Hbcv} implements both versions (BCV1 and BCV2), the
default is BCV1. TO use BCV2, we set \texttt{whichbcv=2}.
Their diagonal counterparts is \texttt{Hbcv.diag}. 
\texttt{Hscv} is the full SCV selector (no diagonal version is available).

<<>>=
(Hlscv1 <- Hlscv(unicef))
(Hlscv2 <- Hlscv.diag(unicef, Hstart=Hlscv1))
(Hbcv1 <- Hbcv(unicef))
(Hbcv2 <- Hbcv(unicef, whichbcv=2))
(Hscv1 <- Hscv(unicef, pre="sphere"))
(Hscv2 <- Hscv(unicef, pre="scale"))

fhat1 <- kde(unicef, Hlscv1)
fhat2 <- kde(unicef, Hlscv2)
fhat3 <- kde(unicef, Hbcv1)
fhat4 <- kde(unicef, Hscv1)
@ 

The plots are in Figure \ref{fig:cv}.
\begin{figure}[!htp]
\begin{center}
<<fig=TRUE>>=
layout(rbind(c(1,2), c(3,4)))
plot(fhat1, main="LSCV")
plot(fhat2, main="LSCV diag")
plot(fhat3, main="BCV")
plot(fhat4, main="SCV")
layout(1)
@
\caption{Cross validation selectors for unicef data}
\label{fig:cv}
\end{center}
\end{figure}


\newpage
\section{More graphics}
There are many options with plotting KDEs, as illustrated in Figure \ref{fig:kde}.

\begin{figure}[!htp]
\begin{center}
<<fig=TRUE>>=
layout(rbind(c(1,2), c(3,4)))
plot(fhat4, display="slice", lcol="blue", ptcol="black", cont=seq(10,90, by=20), cex=0.3)
plot(fhat4, display="slice", ncont=8, drawlabels=FALSE, drawpoints=FALSE)
plot(fhat4, display="image", col=rev(heat.colors(100)))
plot(fhat4, display="persp")
layout(1)
@
\caption{Graphics options for KDE plots}
\label{fig:kde} 
\end{center}
\end{figure}

Slice or contour plots are the default or can be explicitly called by setting
\texttt{display="slice"}. The type of contours plotted is controlled by
\texttt{cont} or \texttt{ncont}: only one of these needs to be set. 
\texttt{cont} takes a vector of percentages and produces a set of contours
at the levels corresponding to the percentages of the maximum height of the 
density estimate. \texttt{ncont} takes a number and \texttt{R} tries to 
compute a pretty set of \texttt{ncont} contours.  
The colour(s) of the contour lines is \texttt{lcol} and
the colour(s) of the plotting symbols is \texttt{ptcol}.
The logical flags \texttt{drawlabels} and \texttt{drawpoints} indicate
whether to draw the labels for the contours levels or the data points.

An alternative to contour plots are the image or heat plots,
called by \texttt{display="image"}.  These 
are similar to contour plots except that the heights of the density 
estimate are represented by different colours rather than with
different level curves. The default colours is \texttt{heat.colors}
but we use instead \texttt{rev(heat.colors} which gives us the high 
values as red and the low values (close to zero) as white with
yellow/orange as intermediate. 

The other alternative is a perspective or wire-frame plot, 
called by \texttt{display="persp"}. This is an attempt
to capture the three-dimensional structure more directly than the 
image or contour plots.


\section{Pre-transformations}
\label{sec:pre}
The plug-in and SCV selector functions use pilot bandwidths of the 
form $\mat{G} = g^2 \mat{I}$ which will be 
clearly inappropriate if the dispersion of the data differs 
markedly between the two coordinate 
directions. Therefore the data should be pre-transformed before the 
algorithm is employed. More 
specifically, we propose that the algorithm is applied to transformed data 
$\vec{X}_1^*, \vec{X}_2^*, 
\dots, \vec{X}_n^*$, where the transformation is either {\em sphering} 
$$\vec{X}^*= \mat{S}^{-1/2} \vec{X}$$
where $\mat{S}$ is the sample covariance matrix of the untransformed data; 
or {\em scaling}
$$\vec{X}^*= \mat{S}_D^{-1/2} \vec{X}$$
where $\mat{S}_D = {\rm diag}(s_{1}^2, s_{2}^2)$ and $s_1^2, s_2^2$ are 
the diagonal elements
of $\mat{S}$ (i.e.~marginal sample variances). The plug-in 
bandwidth matrix $\hat{\HH}^*$
for the sphered or scaled data can be back transformed to the original scale by 
$\hat{\HH} = \mat{S}^{1/2} \hat{\HH}^* \mat{S}^{1/2}$ or 
$\hat{\HH} = 
\mat{S}_D^{1/2} \hat{\HH}^* \mat{S}_D^{1/2}$, as appropriate.


\section{General recommendations}

Like with most other statistical techniques, it is advisable to try a few 
different selectors and visually examine the resulting density estimates. 
One of the primary motivations of the \ks \ package 
is to make visualising kernel density estimates easy.

There are many different bandwidth selectors available in \ks: they may
now pose a problem of \emph{too} much choice.
The full bandwidth selectors will be better than their diagonal counterparts
when the data have large mass oriented obliquely to the co-ordinate axes 
(as is the case for the \texttt{unicef} data). 
Amongst the full selectors there is still a range of performance. In general,
the 2-stage plug-in and the SCV selectors are more stable than the 
1-stage plug-in, BCV and LSCV selectors. The 2-stage plug-in and the SCV selectors
can be viewed as generally recommended bandwidth selectors.



\bibliographystyle{agsm}

\begin{thebibliography}{xx}

\harvarditem{Bowman}{1984}{bowman84}
Bowman, A.~W.  \harvardleft 1984\harvardright , `An alternative method of
  cross-validation for the smoothing of density estimates', {\em Biometrika}
  {\bf 71},~353--360.

\harvarditem{Duong \harvardand\ Hazelton}{2003}{duong03}
Duong, T. \harvardand\ Hazelton, M.~L.  \harvardleft 2003\harvardright ,
  `Plug-in bandwidth matrices for bivariate kernel density estimation', {\em
  Journal of Nonparametric Statistics} {\bf 15},~17--30.

\harvarditem{Duong \harvardand\ Hazelton}{2005}{duong05}
Duong, T. \harvardand\ Hazelton, M.~L.  \harvardleft 2005\harvardright ,
  `Cross-validation bandwidth matrices for multivariate kernel density
  estimation', {\em Scandinavian Journal of Statistics} {\bf 32},~485--506.

\harvarditem[Hall et~al.]{Hall, Marron \harvardand\ Park}{1992}{hall92}
Hall, P., Marron, J.~S. \harvardand\ Park, B.~U.  \harvardleft
  1992\harvardright , `Smoothed cross-validation', {\em Probability Theory and
  Related Fields} {\bf 92},~1--20.

\harvarditem[Jones et~al.]{Jones, Marron \harvardand\ Park}{1991}{jones91b}
Jones, M.~C., Marron, J.~S. \harvardand\ Park, B.~U.  \harvardleft
  1991\harvardright , `A simple root $n$ bandwidth selector', {\em The Annals
  of Statistics} {\bf 19},~1919--1932.

\harvarditem{Rudemo}{1982}{rudemo82}
Rudemo, M.  \harvardleft 1982\harvardright , `Empirical choice of histograms
  and kernel density estimators', {\em Scandinavian Journal of Statistics.
  Theory and Applications} {\bf 9},~65--78.

\harvarditem[Sain et~al.]{Sain, Baggerly \harvardand\ Scott}{1994}{sain94}
Sain, S.~R., Baggerly, K.~A. \harvardand\ Scott, D.~W.  \harvardleft
  1994\harvardright , `Cross-validation of multivariate densities', {\em
  Journal of the American Statistical Association} {\bf 89},~807--817.

\harvarditem{Simonoff}{1996}{simonoff96}
Simonoff, J.~S.  \harvardleft 1996\harvardright , {\em Smoothing Methods in
  Statistics}, Springer-Verlag, New York.

\harvarditem{Wand \harvardand\ Jones}{1993}{wand93}
Wand, M.~P. \harvardand\ Jones, M.~C.  \harvardleft 1993\harvardright ,
  `Comparison of smoothing parameterizations in bivariate kernel density
  estimation', {\em Journal of the American Statistical Association} {\bf
  88},~520--528.

\harvarditem{Wand \harvardand\ Jones}{1994}{wand94}
Wand, M.~P. \harvardand\ Jones, M.~C.  \harvardleft 1994\harvardright ,
  `Multivariate plug-in bandwidth selection', {\em Computational Statistics}
  {\bf 9},~97--116.

\harvarditem{Wand \harvardand\ Jones}{1995}{wand95}
Wand, M.~P. \harvardand\ Jones, M.~C.  \harvardleft 1995\harvardright , {\em
  Kernel Smoothing}, Chapman and Hall Ltd., London.

\end{thebibliography}

\end{document}
